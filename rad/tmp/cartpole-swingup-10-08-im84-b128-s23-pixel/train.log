{"episode_reward": 0.0, "episode": 1.0, "duration": 1.3352458477020264, "step": 500}
{"episode_reward": 154.56731419304836, "episode": 5.0, "duration": 1.166081190109253, "step": 1000}
{"episode_reward": 186.90792292430567, "episode": 9.0, "batch_reward": 0.9180364012718201, "critic_loss": 2.120325517654419, "actor_loss": -2.1030352830886843, "actor_target_entropy": -1.0, "actor_entropy": 0.7160242795944214, "alpha_loss": 0.11541238725185395, "alpha_value": 0.09900026333383705, "duration": 7.560934066772461, "step": 1500}
{"episode_reward": 37.89610614813583, "episode": 13.0, "batch_reward": 0.748314380645752, "critic_loss": 2.326928448677063, "actor_loss": -3.6367076873779296, "actor_target_entropy": -1.0, "actor_entropy": 1.1852403163909913, "alpha_loss": 0.1503256618976593, "alpha_value": 0.09658043969476837, "duration": 8.532882690429688, "step": 2000}
{"episode_reward": 102.13546880898166, "episode": 17.0, "batch_reward": 0.801236093044281, "critic_loss": 3.4939013957977294, "actor_loss": -5.558877563476562, "actor_target_entropy": -1.0, "actor_entropy": 0.9762262821197509, "alpha_loss": 0.13665170669555665, "alpha_value": 0.09427075913545047, "duration": 6.936812162399292, "step": 2500}
{"episode_reward": 208.6266943958171, "episode": 21.0, "batch_reward": 0.8629472613334656, "critic_loss": 5.125540018081665, "actor_loss": -7.608012771606445, "actor_target_entropy": -1.0, "actor_entropy": 0.8888247013092041, "alpha_loss": 0.13128373324871062, "alpha_value": 0.09215083032067617, "duration": 7.692504167556763, "step": 3000}
{"episode_reward": 19.28116448816231, "episode": 25.0, "batch_reward": 0.9083674788475037, "critic_loss": 6.119431400299073, "actor_loss": -9.587699890136719, "actor_target_entropy": -1.0, "actor_entropy": 0.807789659500122, "alpha_loss": 0.12060306817293168, "alpha_value": 0.09014044568810183, "duration": 7.352658033370972, "step": 3500}
{"episode_reward": 200.8224156626127, "episode": 29.0, "batch_reward": 0.9255616664886475, "critic_loss": 8.924103736877441, "actor_loss": -11.394583892822265, "actor_target_entropy": -1.0, "actor_entropy": 0.7928305387496948, "alpha_loss": 0.10409266650676727, "alpha_value": 0.08831990627200693, "duration": 7.570247650146484, "step": 4000}
{"episode_reward": 243.52170959021703, "episode": 33.0, "batch_reward": 1.0218889594078064, "critic_loss": 10.836100959777832, "actor_loss": -14.03088836669922, "actor_target_entropy": -1.0, "actor_entropy": 0.6392269492149353, "alpha_loss": 0.08286692053079606, "alpha_value": 0.08672052395853717, "duration": 16.262922286987305, "step": 4500}
{"episode_reward": 193.48330124213695, "episode": 37.0, "batch_reward": 1.0635849356651306, "critic_loss": 13.505173110961914, "actor_loss": -16.81528663635254, "actor_target_entropy": -1.0, "actor_entropy": 0.6426481246948242, "alpha_loss": 0.06078463159501553, "alpha_value": 0.08539887763167506, "duration": 8.68136739730835, "step": 5000}
{"episode_reward": 267.843358539787, "episode": 41.0, "batch_reward": 1.0841979742050172, "critic_loss": 13.47675895690918, "actor_loss": -20.251073837280273, "actor_target_entropy": -1.0, "actor_entropy": 0.6770703136920929, "alpha_loss": 0.047355037182569504, "alpha_value": 0.08441264985010766, "duration": 8.048341035842896, "step": 5500}
